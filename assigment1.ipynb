{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "assert os.getenv(\"OPENAI_API_KEY\") is not None\n",
    "\n",
    "import gradio as gr\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=HJXWpqpcHik\", language=\"en\")\n",
    "transcript = transcript_loader.load()\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splits = splitter.split_documents(transcript)\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_embedding_model = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(transcript, oai_embedding_model)\n",
    "faiss_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents(docs: list[Document]):    \n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_MESSAGE = \"\"\"Given the context below\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer the following\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're an assistantã€‚\"),\n",
    "        (\"human\", HUMAN_MESSAGE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# now note that we can use the retriever as a runnable\n",
    "# and specifiying that the output of retriever should\n",
    "# be fed into the `context` variable of the prompt\n",
    "chain = {\"context\": faiss_retriever} | prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video discusses creating a YouTube summarizer using AI in less than 10 lines of code. It explains the process of loading transcripts, splitting text into chunks, initializing a language model, and summarizing the transcripts using the \"refine chain\" method. The summary highlights the key steps involved in building the YouTube summarizer and suggests ways to enhance the summarizer further.\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": faiss_retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_4 = chain.invoke(\"Can you summarize the video?\")\n",
    "\n",
    "print(response_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video discusses creating a YouTube summarizer using AI and Lang Chain in less than 10 lines of code. It outlines the process of installing necessary libraries, loading transcripts from a YouTube video, splitting the text into smaller chunks, initializing a large language model (GPT-3.5 Turbo 16k), and summarizing the transcripts using a refine chain method. The tutorial emphasizes utilizing the OpenAI API key, setting up the model parameters, and showcasing the summarization process step by step. Overall, the video provides a comprehensive overview of building a YouTube summarizer with AI technology.\n"
     ]
    }
   ],
   "source": [
    "response_5 = chain.invoke(\"Can you summarize the video?\")\n",
    "print(response_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author used the token text splitter to split the text into smaller chunks.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | faiss_retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"Which text splitter did the author use?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_chat_function(message: str, history):\n",
    "    response = conversational_qa_chain.invoke({\"question\": message, \"chat_history\": []})\n",
    "    return response\n",
    "demo = gr.ChatInterface(my_chat_function, chatbot=gr.Chatbot(height=300), theme=\"soft\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "\n",
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | faiss_retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(), \n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "standalone_question = loaded_memory | standalone_question \n",
    "\n",
    "final_chain = standalone_question | retrieved_documents | answer\n",
    "\n",
    "inputs = {\"question\": \"What splitter is used to split the text?\"}\n",
    "\n",
    "result = final_chain.invoke(inputs)\n",
    "print(result[\"answer\"].content)\n",
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "inputs = {\"question\": \"Can you explain what this splitter does?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "print(result[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "def my_chat_function(message: str, history):\n",
    "    input = {\"question\": message, }\n",
    "    print(standalone_question.invoke(input))\n",
    "    response = final_chain.invoke(input)\n",
    "    memory.save_context(input, {\"answer\": response[\"answer\"].content})\n",
    "    memory.load_memory_variables({})\n",
    "\n",
    "    return response[\"answer\"].content\n",
    "demo = gr.ChatInterface(my_chat_function, chatbot=gr.Chatbot(height=300), theme=\"soft\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".edu_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
